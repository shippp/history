{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35bc6413",
   "metadata": {},
   "source": [
    "# Post Processing\n",
    "\n",
    "This notebook demonstrates a full **post-processing pipeline** for point clouds, generating structured statistics and visualizations.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "The workflow is divided into **11 main steps**:\n",
    "\n",
    "1. **Extract archives** â†’ Uncompress all submissions from `raw_dir` to `extracted_dir`.\n",
    "2. **Analyze submission directory** â†’ Scan `extracted_dir` to collect mandatory files:\n",
    "   - `*_dense_pointcloud.las/laz`\n",
    "   - `*_sparse_pointcloud.las/laz`\n",
    "   - `*_extrinsics.csv`\n",
    "   - `*_intrinsics.csv`\n",
    "3. **Create point cloud symlinks** â†’ Organize dense point cloud files by `(site, dataset)` using symbolic links in `processing_dir`.\n",
    "4. **Convert point clouds to DEMs** â†’ Generate DEMs from point clouds via PDAL; skip existing DEMs unless `overwrite=True`.\n",
    "5. **Coregister DEMs** â†’ Align all DEMs to their reference DEMs; skip existing coregistered DEMs if `overwrite=False`.\n",
    "6. **Generate differential DEMs (DDEMs)** â†’ Compute `ddem_before` and `ddem_after` for each DEM relative to the reference.\n",
    "7. **Compute statistics** â†’ Compute global statistics, coregistration shifts, point cloud metadata, and apply NMAD-based inliers filtering.\n",
    "8. **Compute landcover-based statistics** â†’ Stratify statistics for coregistered DEMs by reference landcover.\n",
    "9. **Generate standard deviation DEMs (std DEMs)** â†’ Compute std DEMs for raw and coregistered DEMs; process both all and inliers-only sets.\n",
    "10. **Compute landcover statistics on std DEMs** â†’ Stratify std DEM statistics by landcover and save results.\n",
    "11. **Generate post-processing plots** â†’ Create comprehensive visual summaries including:\n",
    "    - Global and per-dataset summaries\n",
    "    - DEM quality metrics and NMAD before/after coregistration\n",
    "    - Coregistration shifts and landcover-based statistics\n",
    "    - Slope and hillshade mosaics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a34dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import history.postprocessing.pipeline as pp\n",
    "import history.postprocessing_prefect.flows as ppfl\n",
    "from prefect.task_runners import ConcurrentTaskRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df8f07",
   "metadata": {},
   "source": [
    "## âš™ï¸ Settings and Path Management\n",
    "\n",
    "Main directories used in the post-processing pipeline:\n",
    "\n",
    "- **`raw_dir`** â†’ Contains all raw input archives.  \n",
    "- **`extracted_dir`** â†’ Stores extracted archives.  \n",
    "- **`processing_dir`** â†’ Main workspace for intermediate and processed data.  \n",
    "- **`plots_dir`** â†’ Output folder for generated plots and visualizations.\n",
    "\n",
    "> ðŸ’¡ These directories are automatically created when running the Prefect flows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16210569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "raw_dir = \"/mnt/summer/USERS/DEHECQA/history/output/raw\"\n",
    "extracted_dir = \"/mnt/summer/USERS/DEHECQA/history/output/extracted\"\n",
    "processing_dir = \"/mnt/summer/USERS/DEHECQA/history/output/processing\" \n",
    "plots_dir = \"/mnt/summer/USERS/DEHECQA/history/output/plots\" \n",
    "\n",
    "# other settings\n",
    "OVERWRITE = False\n",
    "DRY_RUN = False # set this to True to avoid process\n",
    "PDAL_EXEC_PATH = \"/home/godinlu/micromamba/envs/pdal/bin/pdal\"\n",
    "MAX_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a09f4",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 1 â€” Uncompress All Submissions\n",
    "\n",
    "This step extracts all supported archive submissions from the `raw_dir` into the `extracted_dir`.  \n",
    "The flow `uncompress_all_submissions` scans the input directory for archives (`.zip`, `.7z`, `.tgz`, `.tar.gz`, `.tar.bz2`, `.tar.xz`) and extracts each one into a corresponding folder in the output directory.\n",
    "\n",
    "Extraction tasks run in parallel via Prefect, and existing folders are skipped unless `overwrite=True` is specified.  \n",
    "Each archive produces one folder in `extracted_dir` containing the uncompressed contents of that submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_runner = ConcurrentTaskRunner(max_workers=4)\n",
    "# ppfl.uncompress_all_submissions.with_options(task_runner=task_runner)(raw_dir, extracted_dir, OVERWRITE)\n",
    "\n",
    "pp.uncompress_all_submissions(raw_dir, extracted_dir, OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b51aa",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 2 â€” Analyze Submission Directory And link files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92318ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.index_submissions_and_link_files(extracted_dir, processing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44e280",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 3 â€” Convert Point Clouds to DEMs\n",
    "\n",
    "In this step, the flow `process_pointclouds_to_dems` converts all point clouds in the `processing_dir` into **Digital Elevation Models (DEMs)** for each site and dataset.\n",
    "\n",
    "For each subdirectory, the flow:\n",
    "\n",
    "- Retrieves the reference DEM for the dataset.\n",
    "- Converts each point cloud into a DEM using the **PDAL** pipeline.\n",
    "- Skips conversion if the DEM already exists and `overwrite=False`.\n",
    "- Executes tasks asynchronously via Prefect, allowing parallel processing.\n",
    "- Logs any errors encountered per file or dataset without stopping the overall execution.\n",
    "\n",
    "All generated DEMs are saved under their corresponding site and dataset subdirectories in `processing_dir`.  \n",
    "The `dry_run` option can be used to simulate the execution without creating any files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f03891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_runner = ConcurrentTaskRunner(max_workers=4)\n",
    "#ppfl.process_pointclouds_to_dems.with_options(task_runner=task_runner)(processing_dir, PDAL_EXEC_PATH, OVERWRITE, DRY_RUN)\n",
    "\n",
    "pp.process_pointclouds_to_dems(processing_dir, PDAL_EXEC_PATH, OVERWRITE, DRY_RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec49b3f",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 4 â€” Coregister DEMs\n",
    "\n",
    "This step aligns all generated DEMs in the `processing_dir` to their corresponding **reference DEMs**.\n",
    "\n",
    "The flow `process_coregister_dems` performs the following for each site and dataset:\n",
    "\n",
    "- Retrieves the reference DEM and its mask.\n",
    "- Aligns all raw DEMs to the reference using the coregistration pipeline.\n",
    "- Skips DEMs that are already coregistered if `overwrite=False`.\n",
    "- Submits each coregistration task asynchronously via Prefect for parallel execution.\n",
    "- Logs any errors for individual DEMs or datasets without stopping the overall workflow.\n",
    "\n",
    "All coregistered DEMs are saved under their respective site and dataset subdirectories in `processing_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_runner = ConcurrentTaskRunner(max_workers=4)\n",
    "# ppfl.process_coregister_dems.with_options(task_runner=task_runner)(processing_dir, OVERWRITE)\n",
    "\n",
    "pp.process_coregister_dems(processing_dir, OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbafaae",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 5 â€” Generate Differential DEMs (DDEMs)\n",
    "\n",
    "In this step, the flow `process_generate_ddems` computes **differential DEMs (DDEMs)** for each dataset in the `processing_dir`.\n",
    "\n",
    "For each site and dataset, the flow:\n",
    "\n",
    "- Computes DDEMs by differencing each DEM against its corresponding reference DEM.\n",
    "- Produces two sets of DDEMs:\n",
    "  - `ddem_before` â†’ from raw DEMs before coregistration.\n",
    "  - `ddem_after` â†’ from coregistered DEMs after alignment.\n",
    "- Executes tasks asynchronously via Prefect for parallel processing.\n",
    "- Skips existing DDEMs if `overwrite=False`.\n",
    "- Logs any errors per file or dataset without interrupting the overall workflow.\n",
    "\n",
    "All generated DDEMs are stored under their respective site and dataset subdirectories in `processing_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_runner = ConcurrentTaskRunner(max_workers=4)\n",
    "# ppfl.process_generate_ddems.with_options(task_runner=task_runner)(processing_dir, OVERWRITE)\n",
    "\n",
    "pp.process_generate_ddems(processing_dir, OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafcd97",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 6 â€” Compute Statistics\n",
    "\n",
    "In this step, the flow `process_compute_statistics` computes **comprehensive statistics** for all DEMs, DDEMs, and point clouds in the `processing_dir`.\n",
    "\n",
    "For each site and dataset, the flow:\n",
    "\n",
    "1. Computes raster statistics for raw DEMs, coregistered DEMs, and DDEMs, either by retrieving existing results or submitting asynchronous tasks.\n",
    "2. Extracts metadata from all point cloud files.\n",
    "3. Computes coregistration shifts for coregistered DEMs.\n",
    "4. Applies an **inliers filter** based on the NMAD of DDEMs using the specified `nmad_multiplier`.\n",
    "5. Saves the resulting statistics as a CSV file within the corresponding subdirectory.\n",
    "\n",
    "All tasks are executed asynchronously via Prefect. Errors for individual computations are logged without interrupting the overall workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_runner = ConcurrentTaskRunner(max_workers=4)\n",
    "# ppfl.process_compute_statistics.with_options(task_runner=task_runner)(processing_dir)\n",
    "\n",
    "pp.process_compute_statistics(processing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f199cec",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 7 â€” Compute Landcover-Based Statistics\n",
    "\n",
    "In this step, the flow `process_compute_landcover_statistics` computes **landcover-stratified statistics** for all coregistered DEMs in the `processing_dir`.\n",
    "\n",
    "For each site and dataset, the flow:\n",
    "\n",
    "1. Retrieves the reference landcover map for the dataset.\n",
    "2. Computes raster statistics for each coregistered DEM, stratified by landcover. Existing statistics are reused if available; otherwise, asynchronous tasks are submitted.\n",
    "3. Aggregates and flattens the results into a single DataFrame per dataset.\n",
    "4. Saves the resulting landcover statistics as a CSV file in the corresponding subdirectory.\n",
    "\n",
    "All computation tasks are executed asynchronously via Prefect. Errors for individual DEMs or datasets are logged without stopping the overall workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_runner = ConcurrentTaskRunner(max_workers=4)\n",
    "# ppfl.process_compute_landcover_statistics.with_options(task_runner=task_runner)(processing_dir)\n",
    "\n",
    "pp.process_compute_landcover_statistics(processing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57bd64",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 8 â€” Generate Standard Deviation DEMs (std DEMs)\n",
    "\n",
    "In this step, the flow `process_generate_std_dems` computes **standard deviation DEMs** for both raw and coregistered DEMs in the `processing_dir`.\n",
    "\n",
    "For each site and dataset, the flow:\n",
    "\n",
    "1. Retrieves the statistics DataFrame to determine available DEMs.\n",
    "2. Prepares two sets for each DEM type: all DEMs and inliers-only DEMs.\n",
    "3. Computes standard deviation DEMs for each set, skipping sets with insufficient DEMs or if outputs already exist and `overwrite=False`.\n",
    "4. Submits asynchronous tasks to generate the std DEMs, logging progress via Prefect.\n",
    "\n",
    "Warnings are issued for sets with insufficient DEMs, and errors in individual tasks are logged without stopping the overall workflow.  \n",
    "All generated std DEMs are saved under the corresponding subdirectory's `std_dems_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bda7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_runner = ConcurrentTaskRunner(max_workers=4)\n",
    "# ppfl.process_generate_std_dems.with_options(task_runner=task_runner)(processing_dir, OVERWRITE)\n",
    "\n",
    "pp.process_generate_std_dems(processing_dir, OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933c74e4",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 9 â€” Compute Landcover-Based Statistics on Standard Deviation DEMs\n",
    "\n",
    "In this step, the flow `process_compute_landcover_statistics_on_std_dems` computes **landcover-stratified statistics** for standard deviation DEMs (std DEMs) in the `processing_dir`.\n",
    "\n",
    "For each site and dataset, the flow:\n",
    "\n",
    "1. Retrieves the reference landcover map for the dataset.\n",
    "2. For both raw and coregistered DEMs, and for each subset ('all' and 'inliers'), computes raster statistics stratified by landcover. Existing statistics are reused when available; otherwise, asynchronous tasks are submitted.\n",
    "3. Aggregates and flattens the results into a single DataFrame per dataset.\n",
    "4. Saves the resulting std landcover statistics as a CSV file in the corresponding subdirectory.\n",
    "\n",
    "All tasks and errors are logged through Prefect. Warnings are issued if no std DEMs are available or if computations fail, without stopping the overall workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ad63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_runner = ConcurrentTaskRunner(max_workers=4)\n",
    "# ppfl.process_compute_landcover_statistics_on_std_dems.with_options(task_runner=task_runner)(processing_dir)\n",
    "\n",
    "pp.process_compute_landcover_statistics_on_std_dems(processing_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a887f65",
   "metadata": {},
   "source": [
    "### ðŸ§© Step 10 â€” Generate Post-Processing Plots\n",
    "\n",
    "In this final step, the flow `generate_postprocessing_plots` creates **visual summaries and plots** for all processed DEMs, DDEMs, and point cloud analyses.\n",
    "\n",
    "For each site and dataset, the flow:\n",
    "\n",
    "- Consolidates statistics and visualization tasks from the `processing_dir`.\n",
    "- Generates a comprehensive set of plots, including:\n",
    "  - Global summaries and per-dataset comparisons.\n",
    "  - DEM quality metrics and NMAD before/after coregistration.\n",
    "  - Coregistration shifts and landcover-based statistics.\n",
    "  - Slope and hillshade mosaics for spatial interpretation.\n",
    "- Saves all plots and visual summaries in the specified `plots_dir`.\n",
    "\n",
    "This step provides a complete visual overview of the post-processing results for analysis and reporting purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f83895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_runner = ConcurrentTaskRunner(max_workers=4)\n",
    "ppfl.generate_postprocessing_plots.with_options(task_runner=task_runner)(processing_dir, plots_dir)\n",
    "\n",
    "#pp.generate_postprocessing_plots(processing_dir, plots_dir, MAX_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189aba8",
   "metadata": {},
   "source": [
    "## Visualize processing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_dir = pp.ProcessingDirectory(processing_dir)\n",
    "processing_dir.submissions_summary()\n",
    "processing_dir.plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
